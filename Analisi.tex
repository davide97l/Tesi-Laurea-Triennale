\section{Analisi dei problemi}
\subsection{Tecniche di computer vision}
La computer vision ha come obiettivo quello di riconoscere e classificare alcuni specifici elementi presenti in un'immagine. Identificare un oggetto significa localizzarne la posizione esatta all'interno dell'immagine. Una volta trovata la sua locazione, l'oggetto viene messo in evidenza disegnando un rettangolo attorno ad esso, detto anche bounding box, in modo che lo racchiuda con la maggiore precisione possibile. La classificazione ha invece come scopo quello individuare la categoria di appartenenza di un oggetto e la probabilità che essa sia realmente quella corretta.
Per classificare un singolo elemento in un'immagine viene tipicamente utilizzata una Convolutional Neural Networks (CNN) allenata con grandi quantità di immagini che possono essere tranquillamente reperite in rete già raggruppate in datasets come ad esempio ImageNet.
La vera sfida salta fuori non appena ci troviamo a dover identificare e classificare nella stessa immagine diversi oggetti appartenenti a categorie diverse, di differenti dimensioni e posizioni e talvolta anche sovrapposti. Questa situazione è molto comune quando ci troviamo ad osservare qualsiasi foto rappresentante il mondo reale. Il risultato ottimale sarebbe quindi di avere una bounding box di dimensioni corrette per ogni oggetto identificato mostrando anche la categoria di appartenenza dell'elemento e la sua probabilità.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/Esempio-computer-vision.jpg}
	\caption{Esempio di un'immagine con box, categoria e probabilità per ogni elemento riconosciuto in essa}
	\label{Esempio di un frame di un video in 4K}
\end{figure}
Il modo più semplice per andare incontro a questo problema è quello di utilizzare una R-CNN (Regional Convolutional Neural Network) la quale tramite un algoritmo greedy chiamato Selective Search estrae delle regioni di interesse nelle quali è probabile che vi sia presente un oggetto. Queste regioni vengono poi date singolarmente in input ad una normale CNN la quale ha il compito di estrarne le caratteristiche principali per poi utilizzare una Support Vector Machine (SVM) presente nell'ultimo strato della CNN per rilevare la presenza di un oggetto ed eventualmente classificarlo. Questo tipo di soluzione presenta il difetto di richiedere molto tempo durante l'operazione di ricerca delle regioni di interesse.\\
La versione più avanzata della R-CNN ed attualmente in uso nei sistemi di computer vision attuali è chiamata Faster R-CNN e risolve il bottleneck della sua antecedente sostituendo la Selective Research con una Region Proposal Network (RPN). Essa prende come input un'immagine di qualsiasi dimensione e restituisce come output un insieme di rettangoli associati ad una probabilità che essi contengano un oggetto o meno. Tramite una CNN viene prima costruita la mappa delle caratteristiche più significative dell'immagine, in seguito viene utilizzata una sliding window per scorrere la mappa delle caratteristiche e darle in input a due fully-connected layers dei quali uno serve per individuare le coordinate del box dell'oggetto\footnote{Box-regression layer} mentre l'altro serve per ritornare la probabilità che nel box vi sia effettivamente un oggetto\footnote{Box-classification layer}. Infine queste regioni vengono passate ad una R-CNN che avrà come al solito il compito di riconoscere e classificare l'oggetto.
\subsection{Computer vision applicata su immagini ad alta risoluzione}
Sebbene sul web sia abbastanza facile reperire grandi quantità di immagini con cui allenare i propri modelli, tuttavia la maggior parte di questi datasets contiene solamente immagini a bassa risoluzione ed è difficile trovare in rete grandi datasets di immagini o video in 4K. Inoltre, la maggior parte dei modelli sono stati progettati per lavorare su immagini a bassa risoluzione (tra i 200 e i 600 pixels) sia per il fatto che una bassa risoluzione è comunque sufficiente per riconoscere e classificare un elemento, sia perchè è più efficiente lavorare su immagini di bassa qualità che su immagini in con una risoluzione molto alta.\\
Lo svantaggio che questo comporta è che nelle immagini in bassa risoluzione si perdono molti dei dettagli che invece potrebbero essere catturati da un immagine o da un video ad alta risoluzione. In aggiunta, i video in 4K o addirittura 8K sono al giorno d'oggi sempre più diffusi perciò anche gli attuali modelli dovranno prima o poi adattarsi per trattare efficacemente immagini di tali risoluzioni. Nella figura sottostante si può vedere un esempio di un frame in alta risoluzione nel quale, diminuendone le dimensioni e perdendo quindi qualità, non sarebbe stato possibile riconoscere alcune delle persone individuate nel frame.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{images/Esempio-4K-video-frame.png}
	\caption{Esempio di object detection in un frame di un video in 4K}
	\label{Esempio di object detection in un frame di un video in 4K}
\end{figure}
\subsection{Frammentazione dell'immagine}
Un'immagine in 4K ha una risoluzione di 3840 x 2160 pixels per un totale di 8294400 di pixels. Una rete neurale in grado di ricevere un input così corposo dovrebbe allenare un numero molto elevato di parametri risultando così in un processo molto lungo e dispendioso tanto che molti dei modelli attuali effettuano un ridimensionamento dell'immagine per adattarla al meglio al loro input. L'idea per aggirare il problema è quella di frammentare l'immagine in diverse sotto-immagini di dimensioni minori e quindi gestibili efficacemente da una singola rete neurale. Questa particolare strategia è chiamata frammentazione dell'immagine e risulta essere molto utile in quanto permette di analizzare un'immagine ad alta risoluzione scomponendola in frammenti di dimensioni minori anziché gestirla nella sua interezza.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{images/esempio-frammentazione.jpg}
	\caption{Esempio di un'immagine in alta risoluzione suddivisa in regioni senza sovrapposizioni}
	\label{Esempio di un'immagine in alta risoluzione frammentata in regioni senza sovrapposizioni}
\end{figure}
Tuttavia questo procedimento non è esente da difficoltà.  Nel caso in cui un oggetto dovesse trovarsi su più regioni diverse, esso potrebbe venire identificato più volte o addirittura essere riconosciuto ogni volta come se fosse un oggetto appartenente ad una categoria diversa.\\
Un primo approccio per risolvere questo problema è quello di porre maggiore attenzione alle labels degli oggetti posizionati in prossimità dei confini delle regioni in quanto con molta probabilità è possibile che l'oggetto continui invece nella regione adiacente piuttosto che essere interamente contenuto nella regione esaminata. Se è quindi presente una label anche in una regione adiacente si passa allora alla verifica che le due labels possano effettivamente appartenere allo stesso elemento. Per fare ciò bisogna assicurarsi che le due labels siano compatibili sia in termini di categoria  che di posizione entro una certa soglia ed in caso affermativo fonderle in una sola label contenente l'oggetto intero.\\
Un' altra soluzione esaminata è quella di suddividere l'immagine intera in regioni con sovrapposizione il cui scopo è quello di generare intersezioni tra labels in prossimità dei confini. In questo modo un elemento contenuto all'interno di un'area di sovrapposizione tra più regioni genererebbe due o più labels quasi completamente sovrapposte. Il problema può essere facilmente gestito con un algoritmo di Non-Maximum Suppression, il quale, per ogni insieme di labels parzialmente sovrapposte e con stessa categoria, tende ad eliminare le labels con probabilità minore tenendo valida solo quella con probabilità massima. Tuttavia il caso più insidioso ed allo stesso tempo più frequente avviene quando due o più labels non solo hanno un'intersezione dentro un'area di sovrapposizione ma la loro box si estende anche al di fuori di esse. E' proprio per far fronte a questo problema che è stato ideato ed implementato un algoritmo il quale verrà descritto nella \textit{sezione 4.1}.
\subsection{Tecniche di object tracking}
Il tracciamento di oggetti in un video è uno di quei problemi presenti nell'ambito dell'informatica che non sono ancora stati risolti con risultati soddisfacenti. Il tracciamento consiste non solo nel localizzare l'oggetto tracciato in una sequenza di frames ma anche nel riconoscere che l'oggetto è sempre lo stesso. Il problema non è per niente banale se pensiamo che in un video uno stesso oggetto può spostarsi, nascondersi dietro qualche altro elemento, deformarsi, cambiare le sue dimensioni, la sua illuminazione, la sua velocità ed il tipo di background. Nel caso ancora peggiore un oggetto tracciato potrebbe addirittura scomparire in un frame per poi ripresentarsi solamente dopo che sono trascorsi un numero casuale di frames. Un algoritmo di tracking ottimale dovrebbe essere in grado di far fronte a tutti questi problemi riconoscendo quindi l'oggetto da esso tracciato tramite per esempio l'assegnazione di un ID univoco. Allo stato dell'arte, questo problema viene affrontando eseguendo inizialmente una normale object detection sul primo frame del video in modo tale da individuarne tutti gli elementi presenti e le rispettive labels. In seguito, ognuno di questi elementi viene assegnato ad un tracker che avrà il compito di tracciare l'elemento nei frames successivi. Tracciare un elemento è un' operazione meno onerosa rispetto alla sua individuazione in quanto il tracker conosce già alcune informazioni relative all'oggetto tracciato acquisite nei frames precedenti potendo così tenere in memoria uno storico del suo stato, come per esempio, le sue ultime locazioni. Per aumentare la sua precisione, un tracker non tiene conto solamente della locazione dell'elemento  osservato ma può anche conservare altre utili informazioni aggiuntive come la sua direzione, una previsione delle locazioni future analizzandone la sua traiettoria oppure può memorizzare un hash\footnote{Al contrario degli hash usati in crittografia, un hash applicato alle immagini è progettato in modo tale che piccole variazioni dei pixels dell'immagine non risultino in un hash molto diverso da quello originale} dei pixels presenti all'interno del bounding box dell'oggetto per poi confrontarlo con l'hash dello stesso oggetto calcolato nel frame successivo. I trackers più performanti come CSK, MOSSE e GOTURN utlizzano alcune delle informazioni sopra descritte per costruire un filtro di correlazione in modo da localizzare l'oggetto nel frame successivo e migliorare la precisione del filtro con le successive individuazioni. Nonostante tutti questi accorgimenti è però inevitabile che col trascorrere dei frames i trackers cominceranno ad essere sempre più imprecisi nell'individuare il loro oggetto tracciato. In particolare, questa situazione può accadere molto velocemente in quei video dove avvengono molti spostamenti e sovrapposizioni tra elementi. E' quindi buona norma aggiornare i trackers con le locazioni corrette effettuando una nuova detection ogni fissato numero di frames. E' qui che si presenta il problema di maggiore rilevanza: una nuova detection effettuata su un nuovo frame non tiene conto delle informazioni acquisite in precedenza in quanto queste con molta probabilità sarebbero errate o imprecise. Il problema è quindi quello di riassegnare a ciascun oggetto individuato lo stesso ID che possedeva in precedenza ed assegnare un nuovo ID ad ogni oggetto comparso nel video per la prima volta. 
\subsection{Object tracking con continue object detections}
Considerata la scarsa affidabilità degli attuali algoritmi di tracciamento, nella soluzione individuata gli oggetti vengono identificati e tracciati effettuando una nuova detection per ogni frame del video. Questo metodo sacrifica l'efficienza del processo per migliorarne l'efficacia. A meno che non venga utilizzato un algoritmo di detection molto veloce come SSD (Single Shot Detector) non è possibile applicare il tracking sui video in tempo reale in quanto il frame rate che ne risulterebbe sarebbe troppo basso. A seguito di una nuova detection, per effettuare una corretta riassegnazione degli ID viene fatto un confronto tra le labels presenti nel frame precedente con quello analizzato con lo scopo di trovare una corrispondenza biunivoca tra due labels e capire quando entrambe si riferiscono allo stesso elemento in modo da trasferirsi l'ID corretto. Anche per realizzare questa soluzione è stato ideato ed implementato un algoritmo descritto nella \textit{sezione 4.2}.
